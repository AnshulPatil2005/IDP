# Titan-Guidance

Titan-Guidance is an automated document analysis platform for contracts and legal documents. It uses FastAPI, Celery, Qdrant, Redis, and OpenRouter (with DeepSeek) for LLM-powered extraction, rule validation, and guidance.

## Tech Stack

- **Backend**: FastAPI (Python 3.11)
- **Database**: SQLAlchemy with PostgreSQL support
- **Task Queue**: Celery with Redis
- **Vector Database**: Qdrant
- **LLM Provider**: OpenRouter (DeepSeek Chat v3)
- **OCR**: docTR (PyTorch-based)
- **Embeddings**: sentence-transformers
- **Frontend**: Nginx (static files)

## Project Structure

```
.env.example
docker-compose.yml
Makefile
README.md
backend/
    app/
        main.py                       # FastAPI entrypoint
        db.py, models.py              # Database models & session
        routes/                       # API endpoints (ingest, clauses, guidance, etc.)
        services/                     # OCR, embeddings, rules, summarizer, etc.
        schemas/                      # Pydantic schemas
        workers/                      # Celery worker config
        assets/prompts/               # LLM system prompts
    scripts/                          # Utility scripts (seed_demo_doc.py, etc.)
    requirements.txt                  # Python dependencies
    Dockerfile                        # Backend container
    alembic.ini                       # Database migrations config
configs/
    rules.yaml                        # Policy rules for validation
infra/
    nginx/, qdrant/                   # Infrastructure configs
```

## Quick Start

### 1. Prerequisites

- Docker & Docker Compose
- Python 3.11 (for local dev)
- OpenRouter API key (for LLM access)

### 2. Environment Setup

Copy `.env.example` to `.env` and add your OpenRouter API key:

```sh
OPENROUTER_API_KEY=your_api_key_here
```

### 3. Build & Run (Docker Compose)

```sh
docker compose up --build
```

Services started:
- **api**: FastAPI backend (`localhost:8000`)
- **worker**: Celery worker
- **redis**: Task queue and cache
- **qdrant**: Vector database for embeddings
- **frontend**: Nginx static file server (`localhost:3000`)

### 4. API Docs

- Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)
- ReDoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)

### 5. Ingest a Document

POST a PDF to `/ingest`:

```sh
curl -F "file=@yourdoc.pdf" http://localhost:8000/ingest
```

### 6. Pipeline

After upload, the pipeline runs:
- OCR → Table extraction → Embeddings → Clause extraction → Deadlines → Rule validation → Summarization → Guidance

### 7. Guidance & Results

- `/docs/{doc_id}/guidance`: Guidance items for a document
- `/docs/{doc_id}/clauses`: Extracted clauses
- `/docs/{doc_id}/deadlines.ics`: Calendar deadlines
- `/ask`: Question answering over documents

## Development

- Backend code: [backend/app/main.py](backend/app/main.py)
- Add rules: [configs/rules.yaml](configs/rules.yaml)
- Seed demo data: `python backend/scripts/seed_demo_doc.py`
- Run migrations: `alembic upgrade head`

## Database Models

The system uses SQLAlchemy with the following main models:
- **Document**: Tracks uploaded documents and processing status
- **Clause**: Extracted contract clauses with type, page, and confidence
- **Guidance**: Generated guidance items with risk levels and actions
- **Deadline**: Extracted deadlines with calendar integration
- **PolicyFire**: Rule violations and policy alerts

## LLM Configuration

The system uses OpenRouter as the LLM provider with DeepSeek Chat v3 (free tier). You can configure this in [docker-compose.yml](docker-compose.yml):

```yaml
LLM_PROVIDER: openrouter
LLM_MODEL: deepseek/deepseek-chat-v3-0324:free
```

## Documentation

- API docs auto-generated by FastAPI
- For code docs, use [Sphinx](https://www.sphinx-doc.org/) in `backend/`

## License

MIT (add your license here)

---

For more details, see the source files:
- [backend/app/main.py](backend/app/main.py)
- [docker-compose.yml](docker-compose.yml)
- [configs/rules.yaml](configs/rules.yaml)
